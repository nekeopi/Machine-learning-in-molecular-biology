{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPiaIyxWPvz1",
        "outputId": "333ff77d-74af-47f4-859c-c1b2fd1675d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX3W6WU3o51J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QDBTeSTnYvR"
      },
      "outputs": [],
      "source": [
        "feature_matrix_train = np.load(\"/content/drive/MyDrive/mlinmb/2/feature_matrix_train.npy\")\n",
        "label_array_train = np.load(\"/content/drive/MyDrive/mlinmb/2/label_array_train.npy\")\n",
        "\n",
        "feature_matrix_valid = np.load(\"/content/drive/MyDrive/mlinmb/2/feature_matrix_valid.npy\")\n",
        "label_array_valid = np.load(\"/content/drive/MyDrive/mlinmb/2/label_array_valid.npy\")\n",
        "\n",
        "feature_matrix_test = np.load(\"/content/drive/MyDrive/mlinmb/2/feature_matrix_test.npy\")\n",
        "label_array_test = np.load(\"/content/drive/MyDrive/mlinmb/2/label_array_test.npy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKny09acoZ4E"
      },
      "source": [
        "1. Use all of the following techniques to train a classifier on the exon-intron data:\n",
        "a. Decision tree (you may use the one you trained in HW2).\n",
        "b. Random forest.\n",
        "c. Extra trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDDNtN5Tofbh",
        "outputId": "a77f5f86-6889-4576-d009-636913f69233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision tree accuracy: 0.8632352216141597\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.83      0.83      7912\n",
            "         1.0       0.89      0.89      0.89     12371\n",
            "\n",
            "    accuracy                           0.86     20283\n",
            "   macro avg       0.86      0.86      0.86     20283\n",
            "weighted avg       0.86      0.86      0.86     20283\n",
            "\n",
            "Random forest accuracy: 0.916185968545087\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.89      0.89      7912\n",
            "         1.0       0.93      0.94      0.93     12371\n",
            "\n",
            "    accuracy                           0.92     20283\n",
            "   macro avg       0.91      0.91      0.91     20283\n",
            "weighted avg       0.92      0.92      0.92     20283\n",
            "\n",
            "\n",
            "Feature importance:\n",
            "Feature 0: 0.01\n",
            "Feature 1: 0.01\n",
            "Feature 2: 0.01\n",
            "Feature 3: 0.01\n",
            "Feature 4: 0.01\n",
            "Feature 5: 0.0\n",
            "Feature 6: 0.04\n",
            "Feature 7: 0.0\n",
            "Feature 8: 0.01\n",
            "Feature 9: 0.03\n",
            "Feature 10: 0.01\n",
            "Feature 11: 0.0\n",
            "Feature 12: 0.01\n",
            "Feature 13: 0.0\n",
            "Feature 14: 0.01\n",
            "Feature 15: 0.02\n",
            "Feature 16: 0.02\n",
            "Feature 17: 0.01\n",
            "Feature 18: 0.01\n",
            "Feature 19: 0.01\n",
            "Feature 20: 0.0\n",
            "Feature 21: 0.01\n",
            "Feature 22: 0.01\n",
            "Feature 23: 0.0\n",
            "Feature 24: 0.04\n",
            "Feature 25: 0.02\n",
            "Feature 26: 0.07\n",
            "Feature 27: 0.01\n",
            "Feature 28: 0.0\n",
            "Feature 29: 0.0\n",
            "Feature 30: 0.02\n",
            "Feature 31: 0.01\n",
            "Feature 32: 0.01\n",
            "Feature 33: 0.02\n",
            "Feature 34: 0.01\n",
            "Feature 35: 0.01\n",
            "Feature 36: 0.01\n",
            "Feature 37: 0.03\n",
            "Feature 38: 0.06\n",
            "Feature 39: 0.01\n",
            "Feature 40: 0.11\n",
            "Feature 41: 0.07\n",
            "Feature 42: 0.01\n",
            "Feature 43: 0.0\n",
            "Feature 44: 0.02\n",
            "Feature 45: 0.01\n",
            "Feature 46: 0.01\n",
            "Feature 47: 0.01\n",
            "Feature 48: 0.02\n",
            "Feature 49: 0.0\n",
            "Feature 50: 0.01\n",
            "Feature 51: 0.01\n",
            "Feature 52: 0.01\n",
            "Feature 53: 0.0\n",
            "Feature 54: 0.01\n",
            "Feature 55: 0.01\n",
            "Feature 56: 0.01\n",
            "Feature 57: 0.01\n",
            "Feature 58: 0.03\n",
            "Feature 59: 0.01\n",
            "Feature 60: 0.02\n",
            "Feature 61: 0.01\n",
            "Feature 62: 0.01\n",
            "Feature 63: 0.03\n",
            "Extra trees accuracy: 0.9138194547157719\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.88      0.89      7912\n",
            "         1.0       0.92      0.94      0.93     12371\n",
            "\n",
            "    accuracy                           0.91     20283\n",
            "   macro avg       0.91      0.91      0.91     20283\n",
            "weighted avg       0.91      0.91      0.91     20283\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Decision Tree\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(feature_matrix_train, label_array_train)\n",
        "dt_predictions = dt_clf.predict(feature_matrix_valid)\n",
        "print(\"Decision tree accuracy:\", accuracy_score(label_array_valid, dt_predictions))\n",
        "print(classification_report(label_array_valid, dt_predictions))\n",
        "\n",
        "dot_data = export_graphviz(\n",
        "    dt_clf,\n",
        "    out_file=None,\n",
        "    feature_names=[f\"Feature {i}\" for i in range(feature_matrix_train.shape[1])],\n",
        "    class_names=[str(i) for i in set(label_array_train)],\n",
        "    filled=True,\n",
        "    rounded=True\n",
        ")\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.view(\"decision_tree\")\n",
        "\n",
        "# Random Forest\n",
        "rf_clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rf_clf.fit(feature_matrix_train, label_array_train)\n",
        "rf_predictions = rf_clf.predict(feature_matrix_valid)\n",
        "print(\"Random forest accuracy:\", accuracy_score(label_array_valid, rf_predictions))\n",
        "print(classification_report(label_array_valid, rf_predictions))\n",
        "\n",
        "print(\"\\nFeature importance:\")\n",
        "for score, name in zip(rf_clf.feature_importances_, [f\"Feature {i}\" for i in range(feature_matrix_train.shape[1])]):\n",
        "    print(f\"{name}: {round(score, 2)}\")\n",
        "\n",
        "# Extra Trees\n",
        "et_clf = ExtraTreesClassifier(random_state=42, n_estimators=100)\n",
        "et_clf.fit(feature_matrix_train, label_array_train)\n",
        "et_predictions = et_clf.predict(feature_matrix_valid)\n",
        "print(\"Extra trees accuracy:\", accuracy_score(label_array_valid, et_predictions))\n",
        "print(classification_report(label_array_valid, et_predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYENlbAPpAJ3"
      },
      "source": [
        "2. Experiment with at least three hyper-parameters controlling an ensemble algorithm (e.g.\n",
        "a random forest) and at least three hyper-parameters controlling component estimators\n",
        "within an ensemble (decision trees).\n",
        "a. For decision trees (standing alone or as a component of an ensemble), experiment\n",
        "with (i) maximum depth, (ii) minimum number of samples for a split, and (iii)\n",
        "minimum number of samples in a leaf.\n",
        "b. For random forests and extra-trees, experiment with (i) number of estimators, (ii)\n",
        "maximum number of features, and (iii) maximum number of samples.\n",
        "c. Use the validation data to evaluate different hyperparameter combinations.\n",
        "d. Select the combination of hyperparameters that results in the highest accuracy on\n",
        "the validation set (this accuracy score should be comparable to the one obtained\n",
        "on the training set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1prBzgwapDve",
        "outputId": "fe95e37b-9463-42f6-a3fe-2dcdfc90677d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best decision tree params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
            "Decision tree validation accuracy: 0.8860622195927624\n",
            "Best Random Forest Params: {'bootstrap': True, 'max_features': None, 'max_samples': 0.8, 'n_estimators': 200}\n",
            "Random Forest Validation Accuracy: 0.9174185278311887\n",
            "Best extra trees params: {'bootstrap': True, 'max_features': None, 'max_samples': None, 'n_estimators': 200}\n",
            "Extra trees validation accuracy: 0.9180101562885175\n",
            "Decision tree Test accuracy: 0.889568132518241\n",
            "Random forest Test accuracy: 0.9205284953658056\n",
            "Extra trees Test accuracy: 0.9230920922894893\n"
          ]
        }
      ],
      "source": [
        "# Decision tree\n",
        "dt_param_grid = {\n",
        "    \"max_depth\": [5, 10, None],\n",
        "    \"min_samples_split\": [20, 100, 200],\n",
        "    \"min_samples_leaf\": [10, 50, 100]\n",
        "}\n",
        "dt_clf = GridSearchCV(DecisionTreeClassifier(random_state=42), dt_param_grid, cv=3, scoring=\"accuracy\")\n",
        "dt_clf.fit(feature_matrix_train, label_array_train)\n",
        "dt_best = dt_clf.best_estimator_\n",
        "dt_predictions = dt_best.predict(feature_matrix_valid)\n",
        "print(\"Best decision tree params:\", dt_clf.best_params_)\n",
        "print(\"Decision tree validation accuracy:\", accuracy_score(label_array_valid, dt_predictions))\n",
        "\n",
        "# Random forest\n",
        "rf_param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
        "    \"max_samples\": [0.8, 0.9, None],\n",
        "    \"bootstrap\": [True]\n",
        "}\n",
        "rf_clf = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=3, scoring=\"accuracy\")\n",
        "rf_clf.fit(feature_matrix_train, label_array_train)\n",
        "rf_best = rf_clf.best_estimator_\n",
        "rf_predictions = rf_best.predict(feature_matrix_valid)\n",
        "print(\"Best Random Forest Params:\", rf_clf.best_params_)\n",
        "print(\"Random Forest Validation Accuracy:\", accuracy_score(label_array_valid, rf_predictions))\n",
        "\n",
        "# Extra trees\n",
        "et_param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
        "    \"max_samples\": [0.8, 0.9, None],\n",
        "    \"bootstrap\": [True]\n",
        "}\n",
        "et_clf = GridSearchCV(ExtraTreesClassifier(random_state=42), et_param_grid, cv=3, scoring=\"accuracy\")\n",
        "et_clf.fit(feature_matrix_train, label_array_train)\n",
        "et_best = et_clf.best_estimator_\n",
        "et_predictions = et_best.predict(feature_matrix_valid)\n",
        "print(\"Best extra trees params:\", et_clf.best_params_)\n",
        "print(\"Extra trees validation accuracy:\", accuracy_score(label_array_valid, et_predictions))\n",
        "\n",
        "# Evaluation on test\n",
        "for name, best_clf in [(\"Decision tree\", dt_best), (\"Random forest\", rf_best), (\"Extra trees\", et_best)]:\n",
        "    test_predictions = best_clf.predict(feature_matrix_test)\n",
        "    print(f\"{name} Test accuracy:\", accuracy_score(label_array_test, test_predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKyzVZJlpt70"
      },
      "source": [
        "3. Merge the training and the validation data, train and optimize a random forest and extra\n",
        "trees using the out-of-bag error. Repeat Step 4, except you do not have a separate\n",
        "validation set. You will use the OOB error to find the best hyper parameter combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuGImw9urW_q",
        "outputId": "dcb1a5d8-1f0d-4f3d-f044-67a31b482750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model 1/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 100}\n",
            "Completed model 1/54, OOB Score: 0.9192541139864584\n",
            "Training model 2/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 200}\n",
            "Completed model 2/54, OOB Score: 0.9203168481714399\n",
            "Training model 3/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 500}\n",
            "Completed model 3/54, OOB Score: 0.9206455288472073\n",
            "Training model 4/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 0.8, 'n_estimators': 100}\n",
            "Completed model 4/54, OOB Score: 0.9193088940990862\n",
            "Training model 5/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 0.8, 'n_estimators': 200}\n",
            "Completed model 5/54, OOB Score: 0.9203825843065934\n",
            "Training model 6/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 0.8, 'n_estimators': 500}\n",
            "Completed model 6/54, OOB Score: 0.9220588557530074\n",
            "Training model 7/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 100}\n",
            "Completed model 7/54, OOB Score: 0.9188377851304862\n",
            "Training model 8/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 200}\n",
            "Completed model 8/54, OOB Score: 0.921072813725705\n",
            "Training model 9/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 500}\n",
            "Completed model 9/54, OOB Score: 0.9222998882485702\n",
            "Training model 10/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.8, 'max_samples': 0.5, 'n_estimators': 100}\n",
            "Completed model 10/54, OOB Score: 0.9195827946622258\n",
            "Training model 11/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.8, 'max_samples': 0.5, 'n_estimators': 200}\n",
            "Completed model 11/54, OOB Score: 0.9209084733878213\n",
            "Training model 12/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.8, 'max_samples': 0.5, 'n_estimators': 500}\n",
            "Completed model 12/54, OOB Score: 0.922256064158468\n",
            "Training model 13/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.8, 'max_samples': 0.8, 'n_estimators': 100}\n",
            "Completed model 13/54, OOB Score: 0.9211385498608585\n",
            "Training model 14/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.8, 'max_samples': 0.8, 'n_estimators': 200}\n",
            "Completed model 14/54, OOB Score: 0.9217192190547143\n",
            "Training model 15/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.8, 'max_samples': 0.8, 'n_estimators': 500}\n",
            "Completed model 15/54, OOB Score: 0.9226614369919145\n",
            "Training model 16/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.8, 'max_samples': 1.0, 'n_estimators': 100}\n",
            "Completed model 16/54, OOB Score: 0.9195937506847514\n",
            "Training model 17/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.8, 'max_samples': 1.0, 'n_estimators': 200}\n",
            "Completed model 17/54, OOB Score: 0.9216096588294586\n",
            "Training model 18/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.8, 'max_samples': 1.0, 'n_estimators': 500}\n",
            "Completed model 18/54, OOB Score: 0.9228915134649517\n",
            "Training model 19/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 100}\n",
            "Completed model 19/54, OOB Score: 0.9189692574007933\n",
            "Training model 20/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 200}\n",
            "Completed model 20/54, OOB Score: 0.9199333873830444\n",
            "Training model 21/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 500}\n",
            "Completed model 21/54, OOB Score: 0.9205250125994259\n",
            "Training model 22/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 0.8, 'n_estimators': 100}\n",
            "Completed model 22/54, OOB Score: 0.9189254333106909\n",
            "Training model 23/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 0.8, 'n_estimators': 200}\n",
            "Completed model 23/54, OOB Score: 0.9200539036308258\n",
            "Training model 24/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 0.8, 'n_estimators': 500}\n",
            "Completed model 24/54, OOB Score: 0.9209084733878213\n",
            "Training model 25/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 100}\n",
            "Completed model 25/54, OOB Score: 0.9188816092205886\n",
            "Training model 26/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 200}\n",
            "Completed model 26/54, OOB Score: 0.9203497162390166\n",
            "Training model 27/54 with params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 500}\n",
            "Completed model 27/54, OOB Score: 0.9209303854328724\n",
            "Training model 28/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 100}\n",
            "Completed model 28/54, OOB Score: 0.9176216666301465\n",
            "Training model 29/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 200}\n",
            "Completed model 29/54, OOB Score: 0.9192431579639327\n",
            "Training model 30/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 500}\n",
            "Completed model 30/54, OOB Score: 0.9195061025045468\n",
            "Training model 31/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 0.8, 'n_estimators': 100}\n",
            "Completed model 31/54, OOB Score: 0.9167232727830489\n",
            "Training model 32/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 0.8, 'n_estimators': 200}\n",
            "Completed model 32/54, OOB Score: 0.9191226417161513\n",
            "Training model 33/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 0.8, 'n_estimators': 500}\n",
            "Completed model 33/54, OOB Score: 0.9201525078335561\n",
            "Training model 34/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 100}\n",
            "Completed model 34/54, OOB Score: 0.9170957775489187\n",
            "Training model 35/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 200}\n",
            "Completed model 35/54, OOB Score: 0.919659486819905\n",
            "Training model 36/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 500}\n",
            "Completed model 36/54, OOB Score: 0.9205140565769003\n",
            "Training model 37/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.8, 'max_samples': 0.5, 'n_estimators': 100}\n",
            "Completed model 37/54, OOB Score: 0.919133597738677\n",
            "Training model 38/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.8, 'max_samples': 0.5, 'n_estimators': 200}\n",
            "Completed model 38/54, OOB Score: 0.9204921445318491\n",
            "Training model 39/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.8, 'max_samples': 0.5, 'n_estimators': 500}\n",
            "Completed model 39/54, OOB Score: 0.9215767907618818\n",
            "Training model 40/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.8, 'max_samples': 0.8, 'n_estimators': 100}\n",
            "Completed model 40/54, OOB Score: 0.918607708657449\n",
            "Training model 41/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.8, 'max_samples': 0.8, 'n_estimators': 200}\n",
            "Completed model 41/54, OOB Score: 0.9203606722615422\n",
            "Training model 42/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.8, 'max_samples': 0.8, 'n_estimators': 500}\n",
            "Completed model 42/54, OOB Score: 0.9216644389420865\n",
            "Training model 43/54 with params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.8, 'max_samples': 1.0, 'n_estimators': 100}\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Merge training and validation datasets\n",
        "feature_matrix_train_merge = np.vstack((feature_matrix_train, feature_matrix_valid))\n",
        "label_array_train_merge = np.hstack((label_array_train, label_array_valid))\n",
        "\n",
        "bagging_param_grid = {\n",
        "    \"n_estimators\": [100, 200, 500],\n",
        "    \"max_samples\": [0.5, 0.8, 1.0],\n",
        "    \"max_features\": [0.5, 0.8, 1.0],\n",
        "    \"bootstrap\": [True],\n",
        "    \"bootstrap_features\": [False, True]\n",
        "}\n",
        "\n",
        "best_oob_score = 0\n",
        "best_params = None\n",
        "best_model = None\n",
        "\n",
        "# Track progress\n",
        "start_time = time.time()\n",
        "total_combinations = len(list(ParameterGrid(bagging_param_grid)))\n",
        "\n",
        "for idx, params in enumerate(ParameterGrid(bagging_param_grid)):\n",
        "    print(f\"Training model {idx + 1}/{total_combinations} with params: {params}\")\n",
        "    model = BaggingClassifier(\n",
        "        DecisionTreeClassifier(),\n",
        "        n_jobs=-1,\n",
        "        oob_score=True,\n",
        "        random_state=42,\n",
        "        **params\n",
        "    )\n",
        "    model.fit(feature_matrix_train_merge, label_array_train_merge)\n",
        "\n",
        "    if model.oob_score_ > best_oob_score:\n",
        "        best_oob_score = model.oob_score_\n",
        "        best_params = params\n",
        "        best_model = model\n",
        "\n",
        "    print(f\"Completed model {idx + 1}/{total_combinations}, OOB Score: {model.oob_score_}\")\n",
        "\n",
        "print(\"Best OOB params:\", best_params)\n",
        "print(\"Best OOB score:\", best_oob_score)\n",
        "\n",
        "# Evaluate on the test\n",
        "y_pred = best_model.predict(feature_matrix_test)\n",
        "test_accuracy = accuracy_score(label_array_test, y_pred)\n",
        "print(\"OOB test accuracy:\", test_accuracy)\n",
        "print(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJkDwaborql7"
      },
      "source": [
        "4. List the most 10 important k-mers using the random-forest classifier and the extra-trees\n",
        "classifier. Are they similar? Or completely different? Provide a reason(s) why they\n",
        "should be similar or different?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu1lj8YssB5U",
        "outputId": "fa4b74e9-56d4-42ee-ef77-a790732ec0c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random forest top 10 feature indices: [40 26 38 41 24 37  6 58  9 30]\n",
            "Extra trees top 10 feature indices: [40 26 38 24  6 37 58 41 30 54]\n",
            "Overlap: 9\n",
            "In common: {26, 37, 38, 6, 40, 41, 24, 58, 30}\n"
          ]
        }
      ],
      "source": [
        "rf_clf = RandomForestClassifier(random_state=42, n_estimators=200)\n",
        "rf_clf.fit(feature_matrix_train_merge, label_array_train_merge)\n",
        "\n",
        "et_clf = ExtraTreesClassifier(random_state=42, n_estimators=200)\n",
        "et_clf.fit(feature_matrix_train_merge, label_array_train_merge)\n",
        "\n",
        "# Feature importances\n",
        "rf_top10 = np.argsort(rf_clf.feature_importances_)[::-1][:10]\n",
        "et_top10 = np.argsort(et_clf.feature_importances_)[::-1][:10]\n",
        "\n",
        "print(\"Random forest top 10 feature indices:\", rf_top10)\n",
        "print(\"Extra trees top 10 feature indices:\", et_top10)\n",
        "\n",
        "# Compare overlap\n",
        "overlap = set(rf_top10).intersection(set(et_top10))\n",
        "print(f\"Overlap: {len(overlap)}\")\n",
        "print(\"In common:\", overlap)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2awHD3MWsIiy"
      },
      "source": [
        "5. Evaluate the final five model (a decision tree, a random forest, extra trees trained on the\n",
        "training set and a random forest, and extra trees trained on the training and the\n",
        "validation sets combined) on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MC9sWgiszCk",
        "outputId": "79bbd57c-6604-48f9-cf1b-e5c32d1df46f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision tree test accuracy: 0.8671859593768487\n",
            "Random forest (train) test accuracy: 0.9217116939459673\n",
            "Extra trees (train) test accuracy: 0.9197396963123644\n",
            "Random forest (train + validation) test accuracy: 0.9230920922894893\n",
            "Extra trees (train + validation) test accuracy: 0.919443896667324\n"
          ]
        }
      ],
      "source": [
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(feature_matrix_train, label_array_train)\n",
        "dt_test_predictions = dt_clf.predict(feature_matrix_test)\n",
        "print(\"Decision tree test accuracy:\", accuracy_score(label_array_test, dt_test_predictions))\n",
        "\n",
        "rf_clf_train = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rf_clf_train.fit(feature_matrix_train, label_array_train)\n",
        "rf_test_predictions_train = rf_clf_train.predict(feature_matrix_test)\n",
        "print(\"Random forest (train) test accuracy:\", accuracy_score(label_array_test, rf_test_predictions_train))\n",
        "\n",
        "et_clf_train = ExtraTreesClassifier(random_state=42, n_estimators=100)\n",
        "et_clf_train.fit(feature_matrix_train, label_array_train)\n",
        "et_test_predictions_train = et_clf_train.predict(feature_matrix_test)\n",
        "print(\"Extra trees (train) test accuracy:\", accuracy_score(label_array_test, et_test_predictions_train))\n",
        "\n",
        "rf_clf_combined = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rf_clf_combined.fit(feature_matrix_train_merge, label_array_train_merge)\n",
        "rf_test_predictions_combined = rf_clf_combined.predict(feature_matrix_test)\n",
        "print(\"Random forest (train + validation) test accuracy:\", accuracy_score(label_array_test, rf_test_predictions_combined))\n",
        "\n",
        "et_clf_combined = ExtraTreesClassifier(random_state=42, n_estimators=100)\n",
        "et_clf_combined.fit(feature_matrix_train_merge, label_array_train_merge)\n",
        "et_test_predictions_combined = et_clf_combined.predict(feature_matrix_test)\n",
        "print(\"Extra trees (train + validation) test accuracy:\", accuracy_score(label_array_test, et_test_predictions_combined))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SvF1LFu1osD"
      },
      "source": [
        "\n",
        "*   First I loaded the data and trained basic random tree, random forest and exta tree and printed their metrics and feature importance for random forest as it was shown in an example in a textbook.\n",
        "\n",
        "*   Then, using gridsearch, I experimented with hyperparameter for each model. Random tree achieved the best accuracy and the best hyperparameters were chosen simirlarly for random forest and random tree, the difference was in number of max_samples.\n",
        "\n",
        "*  Then after merging train and validation sets I run BaggingClassifier to establish the OOB error for the differnet combination of parameters. Every run achieved accuracy of more than 90% and differences were insignificant.\n",
        "\n",
        "\n",
        "*   Then I compared top 10 most important features and found out that 9/10 are the same for random forest and extra trees.\n",
        "\n",
        "\n",
        "*   Finally I trained and evaluated 5 models on train or train + validation sets and the best accuracy was achieved by random forest on mixed sets and visible the lowest by decision tree. That confirms that ensemble learning is beneficiial for prediction accuacy.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeQPJWBCtJb7"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ifni8WxwtKcC"
      },
      "source": [
        "Knowledge:\n",
        "1. Read the slides about ensemble methods up to feature importance.\n",
        "2. Answer the following questions in the same Jupyter notebook that includes the\n",
        "classifiers described above, each in one cell:\n",
        "\n",
        "a. If you have trained five different models on the exact same training data, and\n",
        "they all achieve 95% precision, is there any chance that you can combine these\n",
        "models to get becer results? If so, how? If not, why?\n",
        "\n",
        "b. What is the difference between hard and soft voting classifiers?\n",
        "\n",
        "c. Is it possible to speed up training of a bagging ensemble by distributing it across\n",
        "multiple servers? What about pasting ensembles, boosting ensembles, or random\n",
        "forests?\n",
        "\n",
        "d. What is the benefit of out-of-bag evaluation?\n",
        "\n",
        "e. What makes extra-trees ensembles more random than regular random forests?\n",
        "How can this extra randomness help? Are extra-trees classifiers slower or faster\n",
        "than regular random forests?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIKVmYdGzkI7"
      },
      "source": [
        "a. Yes, by combining their predictions using stacking or voting it is possible to gest better results, becouse of the wisdom of the crow effect.\n",
        "\n",
        "b. Hard voting - the class taht gets the most votes is the predicted one\n",
        "\n",
        "Soft voting - the final prediction is made by averaging probabilities provided by each classifier\n",
        "\n",
        "c. Yes, it is possible to be done in prallel, because they are sampled independantly.\n",
        "\n",
        "d. We dont need a separate validation set.\n",
        "\n",
        "e. They use random tresholds for every feature rather than search for the best one. It is done to minimize the impurity of the split. I think it is faster, because they dont need to optimize the split so much as the regular trees."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
